<!DOCTYPE html>
<html lang="zh-Hans">
    <head>
    <meta charset="utf-8">

    

    <!-- 渲染优化 -->
    <meta name="renderer" content="webkit">
    <meta name="force-rendering" content="webkit">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="HandheldFriendly" content="True" >
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!--icon-->

    
    
    
    
    


    <!-- meta -->


<title>no title | 速查笔记</title>





    <!-- OpenGraph -->
 
    <meta name="description" content="1、概览1、    虚拟化简介在计算机领域，虚拟化指 创建某事物的虚拟(而非实际)版本，包括虚拟的计算机硬件平台、存储设备，以及计算 机网络资源。 虚拟化是一种资源管理技术，它将计算机的各种实体资源(CPU、内存、存储、网络等)予以抽象和转化出来，并提供分割、重新组合，以达到最大化利用物 理资源的目的。 广义来说，我们一直以来对物理硬盘所做的逻辑分区，以及后来的LVM(Logical Volume">
<meta property="og:type" content="article">
<meta property="og:title" content="速查笔记">
<meta property="og:url" content="https://heavyfish.github.io/2020/09/14/Linux/KVM/index.html">
<meta property="og:site_name" content="速查笔记">
<meta property="og:description" content="1、概览1、    虚拟化简介在计算机领域，虚拟化指 创建某事物的虚拟(而非实际)版本，包括虚拟的计算机硬件平台、存储设备，以及计算 机网络资源。 虚拟化是一种资源管理技术，它将计算机的各种实体资源(CPU、内存、存储、网络等)予以抽象和转化出来，并提供分割、重新组合，以达到最大化利用物 理资源的目的。 广义来说，我们一直以来对物理硬盘所做的逻辑分区，以及后来的LVM(Logical Volume">
<meta property="og:locale">
<meta property="og:image" content="file:////Users/shenshawn/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image001.png">
<meta property="og:image" content="file:////Users/shenshawn/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image002.png">
<meta property="og:image" content="https://heavyfish.github.io/Users/shenshawn/Library/Application%20Support/typora-user-images/image-20200914141610369.png">
<meta property="og:image" content="https://heavyfish.github.io/Users/shenshawn/Library/Application%20Support/typora-user-images/image-20200914142649322.png">
<meta property="og:image" content="https://heavyfish.github.io/Users/shenshawn/Library/Application%20Support/typora-user-images/image-20200914142857544.png">
<meta property="og:image" content="https://heavyfish.github.io/Users/shenshawn/Library/Application%20Support/typora-user-images/image-20200914144548365.png">
<meta property="og:image" content="file:////Users/shenshawn/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image005.png">
<meta property="og:image" content="file:////Users/shenshawn/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image006.png">
<meta property="article:published_time" content="2020-09-14T05:58:51.042Z">
<meta property="article:modified_time" content="2020-09-14T07:08:19.287Z">
<meta property="article:author" content="Shenxr">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="file:////Users/shenshawn/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image001.png">


    
<link rel="stylesheet" href="/heavyfish.github.io/css/style/main.css">
 



    
    
        <link rel="stylesheet" id="hl-default-theme" href="/heavyfish.github.io/css/highlight/default.css" media="none" onload="if(getComputedStyle(document.documentElement).getPropertyValue('--color-mode').indexOf('dark')===-1)this.media='all'">
        
    

    
    

     

    <!-- custom head -->

<meta name="generator" content="Hexo 5.3.0"></head>

    <body>
        <div id="app">
            <header class="header">
    <div class="header__left">
        <a href="/heavyfish.github.io/" class="button">
            <span class="logo__text">Demo</span>
        </a>
    </div>
    <div class="header__right">
        
            <div class="navbar__menus">
                
                    <a href="/heavyfish.github.io/" class="navbar-menu button">首页</a>
                
                    <a href="/heavyfish.github.io/tags/" class="navbar-menu button">标签</a>
                
                    <a href="/heavyfish.github.io/categories/" class="navbar-menu button">分类</a>
                
                    <a href="/heavyfish.github.io/archives/" class="navbar-menu button">归档</a>
                
                    <a href="/heavyfish.github.io/friends/" class="navbar-menu button">友链</a>
                
                    <a href="/heavyfish.github.io/page/" class="navbar-menu button">Page</a>
                
            </div>
        
        
        

        
        

        

        
            <a class="dropdown-icon button" id="btn-dropdown" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width='24' height='24' fill="none" stroke="currentColor" stroke-width="0.7" stroke-linecap="round" stroke-linejoin="round"><path fill="currentColor" d="M3.314,4.8h13.372c0.41,0,0.743-0.333,0.743-0.743c0-0.41-0.333-0.743-0.743-0.743H3.314c-0.41,0-0.743,0.333-0.743,0.743C2.571,4.467,2.904,4.8,3.314,4.8z M16.686,15.2H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,15.2,16.686,15.2z M16.686,9.257H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,9.257,16.686,9.257z"></path></svg></a>
            <div class="dropdown-menus" id="dropdown-menus">
                
                    <a href="/heavyfish.github.io/" class="dropdown-menu button">首页</a>
                
                    <a href="/heavyfish.github.io/tags/" class="dropdown-menu button">标签</a>
                
                    <a href="/heavyfish.github.io/categories/" class="dropdown-menu button">分类</a>
                
                    <a href="/heavyfish.github.io/archives/" class="dropdown-menu button">归档</a>
                
                    <a href="/heavyfish.github.io/friends/" class="dropdown-menu button">友链</a>
                
                    <a href="/heavyfish.github.io/page/" class="dropdown-menu button">Page</a>
                
            </div>
        
    </div>
</header>


            <main class="main">
    

<div class="post-title">
    <h1 class="post-title__text">
        no title
    </h1>
    <div class="post-title__meta">
        <a href="/heavyfish.github.io/archives/2020/09/" class="post-meta__date button">2020-09-14</a>
        
 
        
    
    


 

 
    </div>
</div>



<article class="post content-card">
    <div class="post__header"></div>
    <div class="post__content">
        <h1 id="1、概览"><a href="#1、概览" class="headerlink" title="1、概览"></a>1、概览</h1><h2 id="1、-虚拟化简介"><a href="#1、-虚拟化简介" class="headerlink" title="1、    虚拟化简介"></a>1、    虚拟化简介</h2><p>在计算机领域，虚拟化指 创建某事物的虚拟(而非实际)版本，包括虚拟的计算机硬件平台、存储设备，以及计算 机网络资源。</p>
<p>虚拟化是一种资源管理技术，它将计算机的各种实体资源(CPU、内存、存储、网络等)予以抽象和转化出来，并提供分割、重新组合，以达到最大化利用物 理资源的目的。</p>
<p>广义来说，我们一直以来对物理硬盘所做的逻辑分区，以及后来的LVM(Logical Volume Manager)，都可以纳入虚拟化的范畴。</p>
<h2 id="2、-使用虚拟化的原因"><a href="#2、-使用虚拟化的原因" class="headerlink" title="2、    使用虚拟化的原因"></a>2、    使用虚拟化的原因</h2><p>当公司的服务器越来越多，我们需要充分利用资源，也需要统一运维管理，这时虚拟化能帮助我们做很多事。总结如下：</p>
<ol>
<li><p>   打破“一台服务器对应一套应用”的模 式，将物理服务器进行整合，提升利用 率</p>
</li>
<li><p>   服务器和相关IT硬件更少，节省了机房空间，也减少了散热和电力需求</p>
</li>
<li><p>   具备灵活数据备份和应用迁移机制，保障服务永不中断</p>
</li>
<li><p>   资源动态调配和模板化部署，应用系统快速上线，及时响应业务变化。</p>
</li>
</ol>
<p><img src="file:////Users/shenshawn/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image001.png" alt="这里写图片描述"></p>
<h2 id="3、-虚拟化的分类"><a href="#3、-虚拟化的分类" class="headerlink" title="3、    虚拟化的分类"></a>3、    虚拟化的分类</h2><p>软件虚拟化和硬件虚拟化</p>
<ul>
<li>软件虚拟化</li>
<li>硬件虚拟化</li>
</ul>
<p>全虚拟化和半虚拟化</p>
<ul>
<li>全虚拟化</li>
<li>半虚拟化</li>
</ul>
<h3 id="3-1、软件虚拟化"><a href="#3-1、软件虚拟化" class="headerlink" title="3.1、软件虚拟化"></a>3.1、软件虚拟化</h3><p>软件虚拟化，顾名思义，就是通过软件模拟来实现VMM层，通过纯软件的环境来模 拟执行客户机里的指令。</p>
<p>最纯粹的软件虚拟化实现当属QEMU。在没有启用硬件虚拟化辅助的时候，它通过软件的二进制翻译仿真出目标平台呈现给客户机，客户机的每一条目标平台指令都会被 QEMU截取，并翻译成宿主机平台的指令，然后交给实际的物理平台执行。由于每一条都 需要这么操作一下，其虚拟化性能是比较差的，同时其软件复杂度也大大增加。但好处是 可以呈现各种平台给客户机，只要其二进制翻译支持。</p>
<ol>
<li><p>   客户操作系统执行特权指令时，会触发异常（CPU机制，没权限的指令，触发异常）</p>
</li>
<li><p>   然后VMM捕获这个异常，在异常里面做翻译，模拟。并且会缓存翻译结果</p>
</li>
<li><p>   最后返回到客户操作系统内，客户操作系统认为自己的特权指令工作正常，继续运行。</p>
</li>
</ol>
<h3 id="3-2、硬件虚拟化"><a href="#3-2、硬件虚拟化" class="headerlink" title="3.2、硬件虚拟化"></a>3.2、硬件虚拟化</h3><p>硬件虚拟化技术就是指计算机硬件本身提供能力让客户机指令独立执行，而不需要(严格来说是不完全需要)VMM截获重定向。</p>
<p>拿X86 CPU来说，引入了Intel-VT 技术，支持Intel-VT （Intel Virtualization Technology）的CPU，有VMX root operation 和 VMX non-root operation两种模式，两种模式都支持Ring 0 ~ Ring 3 这 4 个运行级别。这下好了，VMM可以运行在VMX root operation模式下，客户OS运行在VMX non-root operation模式下。即是宿主机和虚机是运行在同样硬件上的不同模式，也就说，在硬件这层做了些区分，这样全虚拟化下，有些靠“捕获异常-翻译-模拟”的实现就不需要了。</p>
<h3 id="3-3、半虚拟化"><a href="#3-3、半虚拟化" class="headerlink" title="3.3、半虚拟化"></a>3.3、半虚拟化</h3><p>半虚拟化的思想是：客户机意识到自己是运行在虚拟化环境里，并做相应 修改以配合VMM，配合的方式是可以和VMM约定好的。</p>
<p>这就相当于，我通过修改代码把操作系统移植到一种新的架构上来，就是定制化。所以像XEN这种半虚拟化技术，客户机操作系统都是有一个专门的定制内核版本，和x86、mips、arm这些内核版本等价。这样以来，就不会有捕获异常、翻译、模拟的过程了，性能损耗非常低。这就是XEN这种半虚拟化架构的优势。这也是为什么XEN只支持虚拟化Linux，无法虚拟化windows原因，因为微软不改代码。</p>
<h3 id="3-4、全虚拟化"><a href="#3-4、全虚拟化" class="headerlink" title="3.4、全虚拟化"></a>3.4、全虚拟化</h3><p>与半虚拟化相反的，全虚拟化(Full Virtualization)坚持 客户机 的操作系统完全不需要改动。敏感指令在操作系统和硬件之间被VMM捕捉处理，客户操 作系统无须修改，所有软件都能在虚拟机中运行</p>
<h3 id="3-5、type1和type2虚拟化"><a href="#3-5、type1和type2虚拟化" class="headerlink" title="3.5、type1和type2虚拟化"></a>3.5、type1和type2虚拟化</h3><p>从软件框架的角度上，根据虚拟化层是直接位于硬件之上还是在一个宿主操作系统之上，将虚拟化划分为Typel和Type2，如上图所示。</p>
<p>Type1(类型1)Hypervisor也叫native或bare-metal Hypervisor。这类虚拟化层直接运 行在硬件之上，没有所谓的宿主机操作系统。它们直接控制硬件资源以及客户机。典型地 如Xen 和VMware ESX。</p>
<p>Type2 Hypervisor运行在一个宿主机操作系统之上，如VMware Workstation;或系统里，如KVM。这类Hypervisor通常就是宿主机操作系统的一个应用程 序，像其他应用程序一样受宿主机操作系统的管理。比如VMware Workstation就是运行在 Windows或者Linux操作系统上的一个程序而已[1]。客户机是在宿主机操作系统上的一个 抽象，通常抽象为进程。</p>
<p>将KVM归为Type1或Type2是有争议的，一方面，它是以kernel module的形式加载于 kernel，与kernel融为一体，可以认为它将Linux kernel转变为一个Type1的Hypervisor。另 一方面，在逻辑上，它受制于kernel，所有对硬件资源的管理都是通过kernel去做的，所以 归为Type2</p>
<h2 id="4、cpu的ring"><a href="#4、cpu的ring" class="headerlink" title="4、cpu的ring"></a>4、cpu的ring</h2><p>在介绍三种虚拟化的区别前，首先要明白cpu的ring级别。</p>
<p>Intel的CPU将特权级别分为4个级别：RING0,RING1,RING2,RING3。</p>
<p>RING0级别最高，级别依次往下降。</p>
<p>操作系统（内核）的代码运行在最高运行级别ring0上，可以使用特权指令，控制中断、修改页表、访问设备等等。 </p>
<p>应用程序的代码运行在最低运行级别上ring3上，不能做受控操作。如果要做，比如要访问磁盘，写文件，那就要通过执行系统调用（函数）。</p>
<p>执行系统调用的时候，CPU的运行级别会发生从ring3到ring0的切换，并跳转到系统调用对应的内核代码位置执行，这样内核就为你完成了设备访问，完成之后再从ring0返回ring3。这个过程也称作用户态和内核态的切换。</p>
<p>那么，虚拟化在这里就遇到了一个难题，因为宿主操作系统是工作在ring0的，客户操作系统就不能也在ring0了，但是它不知道这一点，以前执行什么指令，现在还是执行什么指令，那肯定不行啊，没权限啊，玩不转啊。所以这时候虚拟机管理程序（VMM）就要避免这件事情发生。</p>
<p>（VMM在ring0上，一般以驱动程序的形式体现，驱动程序都是工作在ring0上，否则驱动不了设备）</p>
<p><img src="file:////Users/shenshawn/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image002.png" alt="img"></p>
<h1 id="2、KVM"><a href="#2、KVM" class="headerlink" title="2、KVM"></a>2、KVM</h1><h2 id="5、KVM简介"><a href="#5、KVM简介" class="headerlink" title="5、KVM简介"></a>5、KVM简介</h2><p>KVM从诞生开始就定位于基于硬件虚拟化支持的全虚拟化实现。它以内核模块的形 式加载之后，就将Linux内核变成了一个Hypervisor，但硬件管理等还是通过Linux kernel 来完成的，所以它是一个典型的Type 2 Hypervisor</p>
<img src="/Users/shenshawn/Library/Application Support/typora-user-images/image-20200914141610369.png" alt="image-20200914141610369" style="zoom:50%;" />

<p>一个KVM客户机对应于一个Linux进程，每个vCPU则是这个进程下的一个线程，还 有单独的处理IO的线程，也在一个线程组内。所以，宿主机上各个客户机是由宿主机 内核像调度普通进程一样调度的，即可以通过Linux的各种进程调度的手段来实现不同客户机的权限限定、优先级等功能。</p>
<p>客户机所看到的硬件设备是QEMU模拟出来的(不包括VT-d透传的设备)，当客户机对模拟设备进行操作时，由QEMU截获并转换为对实际的物理设备(可能 设置都不实际物理地存在)的驱动操作来完成。</p>
<p>下面介绍一些KVM的功能特性。</p>
<h3 id="1-内存管理"><a href="#1-内存管理" class="headerlink" title="1.内存管理"></a>1.内存管理</h3><p>KVM依赖Linux内核进行内存管理。上面提到，一个KVM客户机就是一个普通的 Linux进程，所以，客户机的“物理内存”就是宿主机内核管理的普通进程的虚拟内存。进 而，Linux内存管理的机制，如大页、KSM(Kernel Same Page Merge，内核的同页合 并)、NUMA(Non-Uniform Memory Arch，非一致性内存架构)、通过mmap的进程间 共享内存，统统可以应用到客户机内存管理上。</p>
<p>早期时候，客户机自身内存访问落实到真实的宿主机的物理内存的机制叫影子页表 (Shadow Page Table)。KVM Hypervisor为每个客户机准备一份影子页表，与客户机自 身页表建立一一对应的关系。</p>
<p>后来再硬件层面上，Intel 的EPT或者AMD的NPT技术 通过一组可以被硬件识别的数据结构，不用KVM建立并维护额外的影子页表，由硬件自动算出GPA→HPA。现在的KVM默 认都打开了EPT/NPT功能</p>
<h3 id="2-存储和客户机镜像的格式"><a href="#2-存储和客户机镜像的格式" class="headerlink" title="2.存储和客户机镜像的格式"></a>2.存储和客户机镜像的格式</h3><p>严格来说，这是QEMU的功能特性。</p>
<p>KVM的原生磁盘格式为QCOW2，它支持快照，允许多级快照、压缩和加密。</p>
<h3 id="3-设备驱动程序"><a href="#3-设备驱动程序" class="headerlink" title="3.设备驱动程序"></a>3.设备驱动程序</h3><p>KVM支持混合虚拟化，其中半虚拟化的驱动程序安装在客户机操作系统中，允许虚拟机使用优化的I/O接口而不使用模拟的设备，从而为网络和块设备提供高性能的I/O。</p>
<p>KVM使用的半虚拟化的驱动程序是IBM和Redhat联合Linux社区开发的<strong>VirtIO</strong>标准; 它是一个与Hypervisor独立的、构建设备驱动程序的接口，允许多种Hypervisor使用一组相 同的设备驱动程序，能够实现更好的对客户机的互操作性。</p>
<p>同时，KVM也支持Intel的VT-d技术，通过将宿主机的PCI总线上的设备透传(pass- through)给客户机，让客户机可以直接使用原生的驱动程序高效地使用这些设备。这种 使用是几乎不需要Hypervisor的介入的。</p>
<h2 id="6、KVM原理"><a href="#6、KVM原理" class="headerlink" title="6、KVM原理"></a>6、KVM原理</h2><h3 id="硬件虚拟化技术"><a href="#硬件虚拟化技术" class="headerlink" title="硬件虚拟化技术"></a>硬件虚拟化技术</h3><h4 id="1-CPU虚拟化"><a href="#1-CPU虚拟化" class="headerlink" title="1.CPU虚拟化"></a>1.CPU虚拟化</h4><p>CPU是计算机系统最核心的模块，我们的程序执行到最后都是翻译为机器语言在CPU 上执行的。在没有CPU硬件虚拟化技术之前，通常使用指令的二进制翻译(binary translation)来实现虚拟客户机中CPU指令的执行，很早期的VMware就使用这样的方案， 其指令执行的翻译比较复杂，效率比较低。所以Intel最早发布的虚拟化技术就是CPU虚拟 化方面的，这才为本书的主角——KVM的出现创造了必要的硬件条件。</p>
<p>Intel在处理器级别提供了对虚拟化技术的支持，被称为VMX(virtual-machine extensions)。有两种VMX操作模式:VMX根操作(root operation)与VMX非根操作 (non-root operation)。作为虚拟机监控器中的KVM就是运行在根操作模式下，而虚拟机 客户机的整个软件栈(包括操作系统和应用程序)则运行在非根操作模式下。进入VMX 非根操作模式被称为“VM Entry”;从非根操作模式退出，被称为“VM Exit”。</p>
<p>VMX的根操作模式与非VMX模式下最初的处理器执行模式基本一样，只是它现在 支持了新的VMX相关的指令集以及一些对相关控制寄存器的操作。VMX的非根操作模式是 一个相对受限的执行环境，为了适应虚拟化而专门做了一定的修改;在客户机中执行的一 些特殊的敏感指令或者一些异常会触发“VM Exit”退到虚拟机监控器中，从而运行在VMX 根模式。正是这样的限制，让虚拟机监控器保持了对处理器资源的控制。</p>
<p>一个虚拟机监控器软件的最基础的运行生命周期及其与客户机的交互如图所示。</p>
<img src="/Users/shenshawn/Library/Application Support/typora-user-images/image-20200914142649322.png" alt="image-20200914142649322" style="zoom:50%;" />

<p>软件通过执行VMXON指令进入VMX操作模式下;在VMX模式下通过VMLAUNCH 和VMRESUME指令进入客户机执行模式，即VMX非根模式;当在非根模式下触发VM Exit时，处理器执行控制权再次回到宿主机的虚拟机监控器上;最后虚拟机监控可以执行 VMXOFF指令退出VMX执行模式。</p>
<p><strong>逻辑处理器在根模式和非根模式之间的切换通过一个叫作VMCS(virtual-machine control data structure)的数据结构来控制;而VMCS的访问是通过VMCS指针来操作的。 VMCS指针是一个指向VMCS结构的64位的地址，使用VMPTRST和VMPTRLD指令对 VMCS指针进行读写，使用MREAD、VMWRITE和VMCLEAR等指令对VMCS实现配置。</strong></p>
<p><strong>对于一个逻辑处理器，它可以维护多个VMCS数据结构，但是在任何时刻只有一个 VMCS在当前真正生效。多个VMCS之间也是可以相互切换的，VMPTRLD指令就让某个 VMCS在当前生效，而其他VMCS就自然成为不是当前生效的。一个虚拟机监控器会为一 个虚拟客户机上的每一个逻辑处理器维护一个VMCS数据结构。</strong></p>
<h4 id="2-内存虚拟化"><a href="#2-内存虚拟化" class="headerlink" title="2.内存虚拟化"></a>2.内存虚拟化</h4><p>内存虚拟化的目的是给虚拟客户机操作系统提供一个从0地址开始的连续物理内存空 间，同时在多个客户机之间实现隔离和调度。在虚拟化环境中，内存地址的访问会主要涉 及以下4个基础概念：</p>
<img src="/Users/shenshawn/Library/Application Support/typora-user-images/image-20200914142857544.png" alt="image-20200914142857544" style="zoom:50%;" />

<p>内存虚拟化就是要将客户机虚拟地址(GVA)转化为最终能够访问的宿主机上的物 理地址(HPA)。</p>
<p>对于客户机操作系统而言，它不感知内存虚拟化的存在，在程序访问客户机中虚拟地址时，通过CR3寄存器可以将其转化为物理地址，但是在虚拟化环境中这个 物理地址只是客户机的物理地址，还不是真实内存硬件上的物理地址。所以，虚拟机监控 器就需要维护从客户机虚拟地址到宿主机物理地址之间的一个映射关系，**在没有硬件提供 的内存虚拟化之前，这个维护映射关系的页表叫作影子页表(Shadow Page Table)**。内存 的访问和更新通常是非常频繁的，要维护影子页表中对应关系会非常复杂，开销也较大。 同时需要为每一个客户机都维护一份影子页表，当客户机数量较多时，其影子页表占用的 内存较大也会是一个问题。</p>
<p><strong>后来，Intel CPU在硬件设计上引入了EPT(Extended Page Tables，扩展页表)，从而将客户机虚拟地址到宿主机物理地址的转换通过硬件来实现</strong>。当然，这个转换是通过两个步骤 来实现的，如图2-3所示。首先，通过客户机CR3寄存器将客户机虚拟地址转化为客户机 物理地址，然后通过查询EPT来实现客户机物理地址到宿主机物理地址的转化。EPT的控制权在虚拟机监控器中，只有当CPU工作在非根模式时才参与内存地址的转换。使用EPT 后，客户机在读写CR3和执行INVLPG指令时不会导致VM Exit，而且客户页表结构自身 导致的页故障也不会导致VM Exit。所以通过引入硬件上EPT的支持，简化了内存虚拟化 的实现复杂度，同时也提高了内存地址转换的效率。</p>
<p>![image-20200914143132211](/Users/shenshawn/Library/Application Support/typora-user-images/image-20200914143132211.png)</p>
<p><strong>除了EPT，Intel在内存虚拟化效率方面还引入了VPID(Virtual-processor identifier)特 性，在硬件级对TLB资源管理进行了优化</strong>。在没有VPID之前，不同客户机的逻辑CPU在 切换执行时需要刷新TLB，而TLB的刷新会让内存访问的效率下降。<strong>VPID技术通过在硬件上为TLB增加一个标志，可以识别不同的虚拟处理器的地址空间，所以系统可以区分虚拟机监控器和不同虚拟机上不同处理器的TLB，在逻辑CPU切换执行时就不会刷新TLB， 而只需要使用对应的TLB即可。</strong>VPID的示意图如图2-4所示。当CPU运行在非根模式下， 且虚拟机执行控制寄存器的“enable VPID”比特位被置为1时，当前的VPID的值是VMCS中 的VPID执行控制域的值，其值是非0的。</p>
<p>![image-20200914143308516](/Users/shenshawn/Library/Application Support/typora-user-images/image-20200914143308516.png)</p>
<h4 id="3-IO虚拟化"><a href="#3-IO虚拟化" class="headerlink" title="3.IO虚拟化"></a>3.IO虚拟化</h4><p>在虚拟化的架构下，虚拟机监控器必须支持来自客户机的I/O请求。通常情况下有以下4种I/O虚拟化方式。</p>
<p>1)设备模拟：在虚拟机监控器中模拟一个传统的I/O设备的特性，比如在QEMU中模拟一个Intel的千兆网卡或者一个IDE硬盘驱动器，在客户机中就暴露为对应的硬件设备。 客户机中的I/O请求都由虚拟机监控器捕获并模拟执行后返回给客户机。（qemu代码实现）</p>
<p>2)前后端驱动接口：在虚拟机监控器与客户机之间定义一种全新的适合于虚拟化环境的交互接口，比如常见的virtio协议就是在客户机中暴露为virtio-net、virtio-blk等网络和磁盘设备，在QEMU中实现相应的virtio后端驱动。（常用，virtio的各种驱动程序去实现）</p>
<p>3)设备直接分配：将一个物理设备，如一个网卡或硬盘驱动器直接分配给客户机使 用，这种情况下I/O请求的链路中很少需要或基本不需要虚拟机监控器的参与，所以性能很好。（Intel CPU的 VT-D技术实现）</p>
<p>4)设备共享分配:其实是设备直接分配方式的一个扩展。在这种模式下，一个(具有特定特性的)物理设备可以支持多个虚拟机功能接口，可以将虚拟功能接口独立地分配 给不同的客户机使用。如SR-IOV就是这种方式的一个标准协议。（Intel CPU的 VT-D技术实现）</p>
<p>前两种都是纯软件的实现，后两种都需要特定硬件特性的支持。</p>
<p>![image-20200914143616219](/Users/shenshawn/Library/Application Support/typora-user-images/image-20200914143616219.png)</p>
<p>设备直接分配在Intel平台上就是VT-d(Virtualization Technology For Directed I/O)特 性，一般在系统BIOS中可以看到相关的参数设置。Intel VT-d为虚拟机监控器提供了几个 重要的能力:I/O设备分配、DMA重定向、中断重定向、中断投递等。图2-5描述了在VT- d硬件特性的帮助下实现的设备直接分配的架构，并与最传统的、通过软件模拟设备的I/O 设备虚拟化进行了对比。</p>
<p>![image-20200914143755548](/Users/shenshawn/Library/Application Support/typora-user-images/image-20200914143755548.png)</p>
<p>尽管VT-d特性支持的设备直接分配方式性能可以接近物理设备在非虚拟化环境中的 性能极限，但是它有一个缺点:单个设备只能分配给一个客户机，而在虚拟化环境下一个宿主机上往往运行着多个客户机，很难保证每个客户机都能得到一个直接分配的设备。为了克服这个缺点，<strong>设备共享分配硬件技术</strong>就应运而生，其中<strong>SR-IOV(Single Root I/O Virtualization and Sharing)就是这样的一个标准</strong>。实现了SR-IOV规范的设备，有一个功能 完整的PCI-e设备成为物理功能(Physical Function，PF)。在使能了SR-IOV之后，PF就 会派生出若干个虚拟功能(Virtual Function，VF)。VF看起来依然是一个PCI-e设备，它 拥有最小化的资源配置，有用独立的资源，可以作为独立的设备直接分配给客户机使用。 Intel的很多高级网卡如82599系列网卡就支持SR-IOV特性，一个85299网卡PF就即可配置 出多达63个VF，基本可满足单个宿主机上的客户机分配使用。当然，<strong>SR-IOV这种特性可 以看作VT-d的一个特殊例子，所以SR-IOV除了设备本身要支持该特性，同时也需要硬件 平台打开VT-d特性支持。</strong>图2-6展示了一个Intel以太网卡支持SR-IOV的硬件基础架构。</p>
<p>![image-20200914144007745](/Users/shenshawn/Library/Application Support/typora-user-images/image-20200914144007745.png)</p>
<h4 id="4-Intel虚拟化技术发展"><a href="#4-Intel虚拟化技术发展" class="headerlink" title="4.Intel虚拟化技术发展"></a>4.Intel虚拟化技术发展</h4><p>Intel硬件虚拟化技术大致分为如下3个类别(这个顺序也基本上是相应技术出现的时 间先后顺序)。</p>
<p>1)VT-x技术:是指Intel处理器中进行的一些虚拟化技术支持，包括CPU中引入的最 基础的VMX技术，使得KVM等硬件虚拟化基础的出现成为可能。同时也包括内存虚拟化 的硬件支持EPT、VPID等技术。</p>
<p>2)VT-d技术:是指Intel的芯片组的虚拟化技术支持，通过Intel IOMMU可以实现对 设备直接分配的支持。</p>
<p>3)VT-c技术:是指Intel的I/O设备相关的虚拟化技术支持，主要包含两个技术:一个 是借助虚拟机设备队列(VMDq)最大限度提高I/O吞吐率，VMDq由Intel网卡中的专用硬 件来完成;另一个是借助虚拟机直接互连(VMDc)大幅提升虚拟化性能，VMDc主要就 是基于SR-IOV标准将单个Intel网卡产生多个VF设备，用来直接分配给客户机。</p>
<p>![image-20200914144329090](/Users/shenshawn/Library/Application Support/typora-user-images/image-20200914144329090.png)</p>
<h3 id="KVM架构概述"><a href="#KVM架构概述" class="headerlink" title="KVM架构概述"></a>KVM架构概述</h3><p>KVM对硬件最低的依赖是CPU的硬件虚拟化支持，比如:Intel的VT技术和AMD 的AMD-V技术，而其他的内存和I/O的硬件虚拟化支持，会让整个KVM虚拟化下的性能 得到更多的提升。</p>
<p>KVM虚拟化的核心主要由以下两个模块组成:</p>
<p><strong>1)KVM内核模块，它属于标准Linux内核的一部分，是一个专门提供虚拟化功能的 模块，主要负责CPU和内存的虚拟化，包括:客户机的创建、虚拟内存的分配、CPU执行 模式的切换、vCPU寄存器的访问、vCPU的执行。</strong></p>
<p><strong>2)QEMU用户态工具，它是一个普通的Linux进程，为客户机提供设备模拟的功能， 包括模拟BIOS、PCI/PCIE总线、磁盘、网卡、显卡、声卡、键盘、鼠标等。同时它通过 ioctl系统调用与内核态的KVM模块进行交互。</strong></p>
<p>在KVM虚拟化架构下，每个客户机就是一个QEMU进程，在一个宿主机上 有多少个虚拟机就会有多少个QEMU进程;客户机中的每一个虚拟CPU对应QEMU进程中 的一个执行线程;一个宿主机中只有一个KVM内核模块，所有客户机都与这个内核模块 进行交互。</p>
<img src="/Users/shenshawn/Library/Application Support/typora-user-images/image-20200914144548365.png" alt="image-20200914144548365" style="zoom:50%;" />

<h4 id="KVM内核模块"><a href="#KVM内核模块" class="headerlink" title="KVM内核模块"></a>KVM内核模块</h4><p>KVM内核模块是标准Linux内核的一部分，由于KVM的存在让Linux本身就变成了一 个Hypervisor，可以原生地支持虚拟化功能。</p>
<p>KVM模块是KVM虚拟化的核心模块，它在内核中由两部分组成:一个是处理器架构 无关的部分，用lsmod命令中可以看到，叫作kvm模块;另一个是处理器架构相关的部 分，在Intel平台上就是kvm_intel这个内核模块。<strong>KVM的主要功能是初始化CPU硬件，打 开虚拟化模式，然后将虚拟客户机运行在虚拟机模式下，并对虚拟客户机的运行提供一定 的支持。</strong></p>
<p>KVM仅支持硬件辅助的虚拟化，所以打开并初始化系统硬件以支持虚拟机的运行， 是KVM模块的职责所在。以KVM在Intel公司的CPU上运行为例，在被内核加载的时候， KVM模块会先初始化内部的数据结构;做好准备之后，KVM模块检测系统当前的CPU， 然后打开CPU控制寄存器CR4中的虚拟化模式开关，并通过执行VMXON指令将宿主操作 系统(包括KVM模块本身)置于CPU执行模式的虚拟化模式中的根模式;最后，<strong>KVM模 块创建特殊设备文件/dev/kvm并等待来自用户空间的命令</strong>。接下来，<strong>虚拟机的创建和运行 将是一个用户空间的应用程序(QEMU)和KVM模块相互配合的过程。</strong></p>
<p><strong>/dev/kvm这个设备可以被当作一个标准的字符设备，KVM模块与用户空间QEMU的 通信接口主要是一系列针对这个特殊设备文件的loctl调用</strong>。当然，每个虚拟客户机针 对/dev/kvm文件的最重要的loctl调用就是“创建虚拟机”。在这里，“创建虚拟机”可以理解 成KVM为了某个特定的虚拟客户机(用户空间程序创建并初始化)创建对应的内核数据 结构。同时，KVM还会返回一个文件句柄来代表所创建的虚拟机。针对该文件句柄的 loctl调用可以对虚拟机做相应的管理，比如创建用户空间虚拟地址和客户机物理地址及真 实内存物理地址的映射关系，再比如创建多个可供运行的虚拟处理器(vCPU)。同样， KVM模块会为每一个创建出来的虚拟处理器生成对应的文件句柄，对虚拟处理器相应的 文件句柄进行相应的loctl调用，就可以对虚拟处理器进行管理。</p>
<p>针对虚拟处理器的最重要的loctl调用就是“执行虚拟处理器”。通过它，用户空间准备 好的虚拟机在KVM模块的支持下，被置于虚拟化模式中的非根模式下，开始执行二进制 指令。在非根模式下，所有敏感的二进制指令都会被处理器捕捉到，处理器在保存现场之 后自动切换到根模式，由KVM决定如何进一步处理(要么由KVM模块直接处理，要么返 回用户空间交由用户空间程序处理)。</p>
<p>除了处理器的虚拟化，内存虚拟化也是由KVM模块实现的，包括前面提到的使用硬 件提供的EPT特性，通过两级转换实现客户机虚拟地址到宿主机物理地址之间的转换。</p>
<p>处理器对设备的访问主要是通过I/O指令和MMIO，其中I/O指令会被处理器直接截 获，MMIO会通过配置内存虚拟化来捕捉。但是，外设的模拟一般不由KVM模块负责。 一般来说，只有对性能要求比较高的虚拟设备才会由KVM内核模块来直接负责，比如虚 拟中断控制器和虚拟时钟，这样可以大量减少处理器模式切换的开销。而大部分的输入输 出设备交给下一节将要介绍的用户态程序QEMU来负责。</p>
<h4 id="QEMU用户态设备模拟"><a href="#QEMU用户态设备模拟" class="headerlink" title="QEMU用户态设备模拟"></a>QEMU用户态设备模拟</h4><p><strong>QEMU原本就是一个著名的开源虚拟机软件项目，而不是KVM虚拟化软件的一部分。</strong></p>
<p>与KVM不同，QEMU最初实现的虚拟机是一个纯软件的实现，通过二进制翻译来实 现虚拟化客户机中的CPU指令模拟，所以性能比较低。但是，其优点是跨平台，甚至可以支持客户机与宿主机并不是 同一个架构(比如在x86平台上运行ARM客户机)。QEMU的代码中有完整的虚拟机实现，包括处理器虚拟化、内存虚拟化，以及KVM 也会用到的虚拟设备模拟(比如网卡、显卡、存储控制器和硬盘等)。</p>
<p>除了二进制翻译的方式，QEMU也能与基于硬件虚拟化的Xen、KVM结合，为它们提 供客户机的设备模拟。通过与KVM的密切结合，让虚拟化的性能提升得非常高，在真实 的企业级虚拟化场景中发挥重要作用，所以我们通常提及KVM虚拟化时就会 说“QEMU/KVM”这样的软件栈。</p>
<p>现在的QEMU已原生 支持KVM虚拟化特性。虚拟机运行期间，QEMU 会通过KVM模块提供的系统调用进入内核，由KVM模块负责将虚拟机置于处理器的特殊 模式下运行。遇到虚拟机进行I/O操作时，KVM模块会从上次的系统调用出口处返回 QEMU，由QEMU来负责解析和模拟这些设备。</p>
<p>从QEMU角度来看，也可以说QEMU使用了KVM模块的虚拟化功能，为自己的虚拟 机提供硬件虚拟化的加速，从而极大地提高了虚拟机的性能。除此之外，虚拟机的配置和 创建，虚拟机运行依赖的虚拟设备，虚拟机运行时的用户操作环境和交互，以及一些针对 虚拟机的特殊技术(如:动态迁移)，都是由QEMU自己实现的。</p>
<p>QEMU除了提供完全模拟的设备(如:e1000网卡、IDE磁盘等)以外，还支持virtio 协议的设备模拟。virtio是一个沟通客户机前端设备与宿主机上设备后端模拟的比较高性 能的协议，在前端客户机中需要安装相应的virtio-blk、virtio-scsi、virtio-net等驱动，而 QEMU就实现了virtio的虚拟化后端。</p>
<h4 id="KVM上层管理工具"><a href="#KVM上层管理工具" class="headerlink" title="KVM上层管理工具"></a>KVM上层管理工具</h4><p>1.libvirt</p>
<p>libvirt是使用最广泛的对KVM虚拟化进行管理的工具和应用程序接口，已经是事实上 的虚拟化接口标准，本节后部分介绍的其他工具都是基于libvirt的API来实现的。作为通 用的虚拟化API，libvirt不但能管理KVM，还能管理VMware、Hyper-V、Xen、VirtualBox 等其他虚拟化方案。</p>
<p>2.virsh</p>
<p>virsh是一个常用的管理KVM虚拟化的命令行工具，对于系统管理员在单个宿主机上 进行运维操作，virsh命令行可能是最佳选择。virsh是用C语言编写的一个使用libvirt API的 虚拟化管理工具，其源代码也是在libvirt这个开源项目中的。</p>
<p>3.virt-manager</p>
<p>virt-manager是专门针对虚拟机的图形化管理软件，底层与虚拟化交互的部分仍然是 调用libvirt API来操作的。virt-manager除了提供虚拟机生命周期(包括:创建、启动、停止、打快照、动态迁移等)管理的基本功能，还提供性能和资源使用率的监控，同时内置 了VNC和SPICE客户端，方便图形化连接到虚拟客户机中。virt-manager在RHEL、 CentOS、Fedora等操作系统上是非常流行的虚拟化管理软件，在管理的机器数量规模较小 时，virt-manager是很好的选择。因其图形化操作的易用性，成为新手入门学习虚拟化操 作的首选管理软件。</p>
<p>4.OpenStack</p>
<p>OpenStack是一个开源的基础架构即服务(IaaS)云计算管理平台，可用于构建共有 云和私有云服务的基础设施。OpenStack是目前业界使用最广泛的功能最强大的云管理平 台，它不仅提供了管理虚拟机的丰富功能，还有非常多其他重要管理功能，如:对象存 储、块存储、网络、镜像、身份验证、编排服务、控制面板等。OpenStack仍然使用libvirt API来完成对底层虚拟化的管理。</p>
<h2 id="8、-kvm配置"><a href="#8、-kvm配置" class="headerlink" title="8、 kvm配置"></a>8、 kvm配置</h2><h3 id="8-1、安装软件包"><a href="#8-1、安装软件包" class="headerlink" title="8.1、安装软件包"></a>8.1、安装软件包</h3><p>复杂版：yum install qemu-kvm qemu-img virt-manager libvirt libvirt-<a target="_blank" rel="noopener" href="https://www.centoschina.cn/go?url=http://lib.csdn.net/base/11"><strong>Python</strong></a> ibvirt-client virt-install virt-viewer bridge-utils</p>
<p>简介版：yum install qemu-kvm qemu-kvm-tools libvirt</p>
<p>qemu-kvm：qemu模拟器</p>
<p>qemu-img：qemu的磁盘管理器</p>
<p>virt-install：用来创建虚拟机的命令行工具</p>
<p>virt-manager：GUI虚拟机管理工具</p>
<p>libvirt：提供libvirtd daemon来管理虚拟机和控制hypervisor</p>
<p>libvirt-Python：基于libvirt API的python语言绑定工具包，通过该包可以实现对KVM日常管理和监控数据的获取</p>
<p>libvirt-client：提供客户端API用来访问server和提供管理虚拟机命令行工具的virsh实体</p>
<p>virt-install：是一个命令行工具，它能够为KVM、Xen或其它支持libvrit API的hypervisor创建虚拟机并完成GuestOS安装</p>
<p>virt-viewer：图形控制台</p>
<p>bridge-utils：创建和管理桥接设备的工具</p>
<h3 id="8-2、检查内核模块嵌入"><a href="#8-2、检查内核模块嵌入" class="headerlink" title="8.2、检查内核模块嵌入"></a>8.2、检查内核模块嵌入</h3><p># 检查嵌入</p>
<p>lsmod | grep kvm</p>
<p>kvm_intel 55496 0 </p>
<p>kvm 337900 1 kvm_intel </p>
<p># 嵌入命令</p>
<p>modprobe kvm</p>
<p>modprobe kvm-intel</p>
<h3 id="8-3、创建安装盘"><a href="#8-3、创建安装盘" class="headerlink" title="8.3、创建安装盘"></a>8.3、创建安装盘</h3><p><a target="_blank" rel="noopener" href="https://www.hongweipeng.com/index.php/archives/1368/">qemu支持的硬盘格式</a></p>
<p>qemu-img create –f qcow2 /xx/xx/name 10G</p>
<h3 id="8-4、创建虚拟机"><a href="#8-4、创建虚拟机" class="headerlink" title="8.4、创建虚拟机"></a>8.4、创建虚拟机</h3><p>virt-install –virt-type kvm \</p>
<p>–name CentOS-7 \</p>
<p>–ram 1024 \</p>
<p>–vcpus 1 \</p>
<p>–cdrom=/data/kvmtest/CentOS-7-x86_64-Minimal-1804.iso \</p>
<p>–disk path=/data/kvmtest/CentOS-7.qcow2 \</p>
<p>–network network=default \</p>
<p>–graphics vnc,listen=0.0.0.0 –noautoconsole \</p>
<p>–os-type=linux \</p>
<p>–os-variant=rhel7</p>
<h4 id="virt-install参数详解"><a href="#virt-install参数详解" class="headerlink" title="virt-install参数详解"></a>virt-install参数详解</h4><p>–virt-type：使用的hypervisor，如kvm、qemu、xen等</p>
<p>-n NAME, –name=NAME：虚拟机名称，需全局惟一；</p>
<p>-r MEMORY, –ram=MEMORY：虚拟机内在大小，单位为MB； </p>
<p>–vcpus=VCPUS[,maxvcpus=MAX][,sockets=#][,cores=#][,threads=#]：VCPU个数及相关配置；</p>
<p>-c CDROM, –cdrom=CDROM：光盘安装介质；</p>
<p>–disk=DISKOPTS：指定存储设备及其属性；格式为–disk /some/storage/path,opt1=val1，opt2=val2等； </p>
<p>常用的选项有： </p>
<p>l device：设备类型，如cdrom、disk或floppy等，默认为disk；</p>
<p>l bus：磁盘总结类型，其值可以为ide、scsi、usb、virtio或xen； </p>
<p>l perms：访问权限，如rw、ro或sh（共享的可读写），默认为rw；</p>
<p>l size：新建磁盘映像的大小，单位为GB；</p>
<p>l cache：缓存模型，其值有none、writethrouth（缓存读）及writeback（缓存读写）；</p>
<p>l format：磁盘映像格式，如raw、qcow2、vmdk等； </p>
<p>l sparse：磁盘映像使用稀疏格式，即不立即分配指定大小的空间； </p>
<p>l –nodisks：不使用本地磁盘，在LiveCD模式中常用；</p>
<p>–network=NETWORK：将虚拟机连入宿主机的网络中，其中NETWORK可以为：</p>
<p>l bridge=BRIDGE：连接至名为“BRIDEG”的桥设备；</p>
<p>l network=NAME：连接至名为“NAME”的网络；</p>
<p>–graphics TYPE,opt1=val1,opt2=val2：指定图形显示相关的配置，此选项不会配置任何显示硬件（如显卡），而是仅指定虚拟机启动后对其进行访问的接口；</p>
<p>TYPE：指定显示类型，可以为vnc、sdl、spice或none等，默认为vnc； 如果选择spice，则需要linux图形界面的支持</p>
<p>port： TYPE为vnc或spice时其监听的端口； </p>
<p>listen：TYPE为vnc或spice时所监听的IP地址，默认为127.0.0.1，可以通过修改/etc/libvirt/qemu.conf定义新的默认值； </p>
<p>password：TYPE为vnc或spice时，为远程访问监听的服务进指定认证密码；</p>
<p>–noautoconsole：禁止自动连接至虚拟机的控制台；</p>
<p>–os-type=DISTRO_TYPE：操作系统类型，如Linux、unix或windows等； </p>
<p>–os-variant=DISTRO_VARIANT：某类型操作系统的变体，如rhel5、fedora8等</p>
<p><strong># 不常用的，了解</strong></p>
<p>–cpu=CPU：CPU模式及特性，如coreduo等；可以使用qemu-kvm -cpu ?来获取支持的CPU模式；</p>
<p>-l LOCATION, –location=LOCATION：安装源URL，支持FTP、HTTP及NFS等，如<a href="ftp://172.16.0.1/pub%EF%BC%9B">ftp://172.16.0.1/pub；</a></p>
<p>–pxe：基于PXE完成安装； –livecd: 把光盘当作LiveCD； </p>
<p>-x EXTRA, –extra-args=EXTRA：根据–location指定的方式安装GuestOS时，用于传递给内核的额外选项，例如指定kickstart文件的位置，</p>
<p>–extra-args “ks=<a target="_blank" rel="noopener" href="http://172.16.0.1/class.cfg&quot;">http://172.16.0.1/class.cfg&quot;</a> </p>
<p>–boot=BOOTOPTS：指定安装过程完成后的配置选项，如指定引导设备次序、使用指定的而非安装的kernel/initrd来引导系统启动等 ；例如： –boot cdrom,hd,network：指定引导次序； </p>
<p>–boot kernel=KERNEL,initrd=INITRD,kernel_args=”console=/dev/ttyS0”：指定启动系统的内核及initrd文件；</p>
<h3 id="8-5、VNC安装"><a href="#8-5、VNC安装" class="headerlink" title="8.5、VNC安装"></a>8.5、VNC安装</h3><p>因为上面指定的是用VNC显示图形界面。所以下载一个VNC viewer，连上IP:5900，进行图形化安装。安装完成后就可以正常操作了</p>
<h3 id="8-6、安装电源模块"><a href="#8-6、安装电源模块" class="headerlink" title="8.6、安装电源模块"></a>8.6、安装电源模块</h3><p>yum install acpid</p>
<p>ACPI是Advanced Configuration and PowerInterface缩写，高级配置和电源管理接口。</p>
<p>acpid中的d则代表daemon。Acpid是一个<a target="_blank" rel="noopener" href="http://baike.baidu.com/view/4274331.htm">用户空间</a>的服务进程，它充当linux<a target="_blank" rel="noopener" href="http://baike.baidu.com/view/1366.htm">内核</a>与应用程序之间通信的接口，负责将kernel中的电源管理事件转发给应用程序。</p>
<p>如果没有这个服务，是不能virsh shutdown 掉虚机的</p>
<h3 id="8-7、KVM的xml文件"><a href="#8-7、KVM的xml文件" class="headerlink" title="8.7、KVM的xml文件"></a>8.7、KVM的xml文件</h3><p>xml文件是虚机的配置文件。里面有虚机名，磁盘大小，vcpu，ram，networ等等的信息。我们通过xml文件可以了解到虚机的整个基础配置。还可以通过复制xml文件，用define命令生成新的虚机</p>
<p><domain type = 'kvm'>     //虚拟机类型，kvm</p>
<p> <name>demo</name>    //虚拟机名称</p>
<p> <memory>1048576</memory> //分配内存，单位kb</p>
<p> <vcpu>1</vcpu>      //分配vcpu，单位个数</p>
 <os>

<p>  &lt;type arch = ‘x86_64’machine = ‘pc’&gt;hvm</type></p>
<p>  &lt;bootdev = ‘cdrom’/&gt; //cd 启动</p>
<p>  &lt;bootdev = ‘hd’/&gt;   //硬盘启动</p>
 </os>

 <features>

  <acpi/>

  <apic/>

  <pae/>

 </features>

 <clock offset = 'localtime'/>

<p> <on_poweroff>destroy</on_poweroff></p>
<p> <on_reboot>restart</on_reboot></p>
<p> <on_crash>destroy</on_crash></p>
 <devices>

<p>  <emulator>/usr/bin/kvm</emulator></p>
<p>  &lt;disk type = ‘file’device = ‘disk’&gt;  //对应的镜像，就是之前使用qemu-img命令新建的img文件，注意路径要正确</p>
<p>   &lt;driver name = ‘qemu’type = ‘raw’/&gt;</p>
<p>   &lt;sourcefile = ‘/var/lib/lynn/img/template.img’/&gt;</p>
<p>   &lt;target dev = ‘hda’bus = ‘ide’/&gt;</p>
  </disk>

<p>  &lt;disk type = ‘file’device = ‘cdrom’&gt; //可选项，iso通常是操作系统的安装光盘</p>
   <source file = '/var/lib/lynn/img/template.iso'/>

<p>   &lt;target dev = ‘hdb’bus = ‘ide’/&gt;</p>
  </disk>

<p>  <interface type = 'bridge'>      //libvirt默认虚拟机的网络配置是NAT模式，就是虚拟机与宿主机的网络拓扑是NAT形式。实际中，许多开发者更希望使用网桥模式。</p>
   <source bridge = 'br0'/>

  </interface>

<p>  &lt;input type =’tablet’bus=’usb’/&gt;</p>
<p>  &lt;input type = ‘mouse’bus = ‘ps2’/&gt;</p>
<p>  &lt;graphics type = ‘vnc’port = ‘-1’listen = ‘0.0.0.0’keymap = ‘en-us’/&gt; //vnc端口系统自动配置</p>
 </devices>

</domain>



<h3 id="8-8、网络配置"><a href="#8-8、网络配置" class="headerlink" title="8.8、网络配置"></a>8.8、网络配置</h3><h4 id="1、将网络由vibr0桥接，变为br0桥接"><a href="#1、将网络由vibr0桥接，变为br0桥接" class="headerlink" title="1、将网络由vibr0桥接，变为br0桥接"></a>1、将网络由vibr0桥接，变为br0桥接</h4><p>在最初创建虚机时，设置虚机的网络是“network=default”。让虚机连入名为default的网络。当时我们也可设置“network=bridge_name”,让虚机连入桥设备。但我们没有这样做，所以现在虚机是连在virbr0桥设备上的。</p>
<h5 id="1）-vibr0是什么"><a href="#1）-vibr0是什么" class="headerlink" title="1） vibr0是什么"></a>1） vibr0是什么</h5><p>vibr0是KVM默认创建的一个Bridge,它为连接其上的虚机网卡提供NAT访问外网的功能。virbr0默认会被分配一个IP地址，并为连接其上的其他KVM虚拟网卡提供DHCP服务（dnsmasq服务）。此时，虚机网络为NAT模式，访问外部网络简单，但外部网络不能访问进来</p>
<h5 id="2）-br0桥接"><a href="#2）-br0桥接" class="headerlink" title="2） br0桥接"></a>2） br0桥接</h5><p>这里的br0桥接，就是将虚机的网络模式变为桥接模式。这样虚机与宿主机处于同一网络环境，类似于一台真实的宿主机。操作如下：</p>
<p><strong>（1）</strong>   <strong>宿主机操作</strong></p>
<p>brctl show #查看桥接连接</p>
<p>brctl addbr br0 #新建桥接连接</p>
<p>brctl addif br0 eth0 #将eth0网卡，绑到br0桥设备上</p>
<p>ip addr del dev eth0 192.168.56.11/24 #删除eth0网卡的IP地址</p>
<p>ifconfig br0 192.168.56.11/24 up #将eth0网卡的IP，配到br0上</p>
<p>route add default gw 192.168.56.2 #配置默认网关</p>
<p>修改KVM的xml文件（把端口类型改为bridge，源端口改为br0）</p>
<p><img src="file:////Users/shenshawn/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image005.png" alt="img"></p>
<p><strong>（2）</strong>   <strong>进入KVM**</strong>虚机，为其配置网卡IP**</p>
<p>BOOTPROTO=staic</p>
<p> IPADDR=192.168.56.111 （与宿主机同网段）</p>
<p> NETMASK=255.255.255.0</p>
<p> GATEWAY=192.168.56.2 </p>
<h4 id="2、创建VLAN"><a href="#2、创建VLAN" class="headerlink" title="2、创建VLAN"></a>2、创建VLAN</h4><p>vconfig add eth0 10 # 创建VLAN (/proc/net/vlan/config)10，并将eth0加入VLAN</p>
<p>brctl addbr br10   # 创建网桥</p>
<p>brctl addif br10 eth0.10 # 将eth0绑到网桥</p>
<p>brctl addif br10 vnet0  # 将KVM虚机的网卡 加入网桥br10</p>
<h4 id="3、参考文档"><a href="#3、参考文档" class="headerlink" title="3、参考文档"></a>3、参考文档</h4><p>Linux系统配置kvm+vlan </p>
<p>(<a target="_blank" rel="noopener" href="http://blog.51cto.com/fklinux/2045498">http://blog.51cto.com/fklinux/2045498</a>)</p>
<p>KVM虚拟机网络配置 Bridge方式，NAT方式 </p>
<p>(<a target="_blank" rel="noopener" href="https://blog.csdn.net/hzhsan/article/details/44098537/">https://blog.csdn.net/hzhsan/article/details/44098537/</a>)</p>
<p>KVM 虚拟机的网络模式学习及配置</p>
<p>(<a target="_blank" rel="noopener" href="https://www.baidu.com/link?url=Cs-jCTF5gY6F3T73jhTWpR129jJwRnG2JqdsLZQSs2amaVgk8lf2LVjV9jevyzMdA1NpVzHja6-0riqJJZ7ZgK7AUj92XFzATOo48O441wy&amp;wd=&amp;eqid=b8fdc22300032726000000055b053d6f">https://www.baidu.com/link?url=Cs-jCTF5gY6F3T73jhTWpR129jJwRnG2JqdsLZQSs2amaVgk8lf2LVjV9jevyzMdA1NpVzHja6-0riqJJZ7ZgK7AUj92XFzATOo48O441wy&amp;wd=&amp;eqid=b8fdc22300032726000000055b053d6f</a>)</p>
<h3 id="8-7、KVM与宿主机的关系"><a href="#8-7、KVM与宿主机的关系" class="headerlink" title="8.7、KVM与宿主机的关系"></a>8.7、KVM与宿主机的关系</h3><p>KVM在Linux中是一个进程的表现形式，它受的到CPU的调度</p>
<p>Libvirt是管理kvm的工具，关掉Libvird服务，不影响KVM虚机的正常运行，只是无法用libvirt管理而已</p>
<h2 id="9、-virsh命令"><a href="#9、-virsh命令" class="headerlink" title="9、 virsh命令"></a>9、 virsh命令</h2><p>virsh命令，调用libvid，Libvirtd调用qemu-kvm操作虚拟机。通过virsh命令，我们可以实现在CLI对虚拟机的管理</p>
<p>virsh有命令模式和交互模式。</p>
<p>u 命令模式：直接在virsh后面接参数</p>
<p>u 交互模式：直接写virsh。</p>
<p>u 事实上交互模式和命令的参数都是一样的</p>
<h3 id="9-1、常用命令"><a href="#9-1、常用命令" class="headerlink" title="9.1、常用命令"></a>9.1、常用命令</h3><table>
<thead>
<tr>
<th>命令(前缀 virsh)</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>list</td>
<td>显示显示本地活动虚拟机  -all：显示本地所有的虚拟机</td>
</tr>
<tr>
<td>creat KVM_name.xml</td>
<td>创建虚机（创建后，虚机立即执行。虚机关机后自动消失）</td>
</tr>
<tr>
<td>define KVM_name.xml</td>
<td>通过配置文件定义一个虚机。（定以后虚机不是活动的）</td>
</tr>
<tr>
<td>start KVM_name</td>
<td>启动虚机</td>
</tr>
<tr>
<td>console KVM_name</td>
<td>连接虚机 【<a target="_blank" rel="noopener" href="https://www.cnblogs.com/xieshengsen/p/6215168.html">KVM 通过virsh console连入虚拟机</a>】</td>
</tr>
<tr>
<td>suspend KVM_name</td>
<td>暂停虚机，虚拟机处于paused暂停状态  但是仍然消耗资源,只不过不被超级管理程序调度而已。</td>
</tr>
<tr>
<td>resume KVM_name</td>
<td>唤醒虚机</td>
</tr>
<tr>
<td>shutdown KVM_name</td>
<td>正常关闭虚机</td>
</tr>
<tr>
<td>destroy KVM_name</td>
<td>强制关闭虚机，并删除  libvirt直接销毁demo，取消demo的定义</td>
</tr>
<tr>
<td>undefine KVM_name</td>
<td>移除虚拟机  在虚拟机处于Running状态时，调用该指令，该指令暂时不生效  但是当虚拟机被关闭后，该指令生效移除该虚拟机，也可以在该指令生效之前调用define+TestKVM.xml取消该指令</td>
</tr>
</tbody></table>
<h3 id="9-2、显示虚机信息命令"><a href="#9-2、显示虚机信息命令" class="headerlink" title="9.2、显示虚机信息命令"></a>9.2、显示虚机信息命令</h3><table>
<thead>
<tr>
<th>命令（前缀为 virsh）</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>dominfo KVM_name</td>
<td>显示虚机的基本信息</td>
</tr>
<tr>
<td>dumpxml KVM_name</td>
<td>显示虚机的当前配置文件</td>
</tr>
<tr>
<td>dump KVM_name file</td>
<td>将虚机的配置文件重定向到file</td>
</tr>
<tr>
<td>domiflist KVM_name</td>
<td>列出虚机网卡接口</td>
</tr>
<tr>
<td>domifstat KVM_name vnet0</td>
<td>显示网卡信息</td>
</tr>
<tr>
<td>domuuid KVM_name</td>
<td>显示虚机UUID</td>
</tr>
<tr>
<td>domid KVM_name/UUID</td>
<td>显示虚机ID</td>
</tr>
<tr>
<td>cpu-stats KVM_name</td>
<td>显示虚机CPU状态</td>
</tr>
<tr>
<td>vncdisplay</td>
<td>显示虚机的IP</td>
</tr>
</tbody></table>
<h3 id="9-3、更改虚机"><a href="#9-3、更改虚机" class="headerlink" title="9.3、更改虚机"></a>9.3、更改虚机</h3><table>
<thead>
<tr>
<th>命令（前缀为 virsh）</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>exit KVM_name</td>
<td>编辑虚机的配置文件</td>
</tr>
<tr>
<td>setmen KVM_name size</td>
<td>设置虚机内存。要求虚机不活动</td>
</tr>
<tr>
<td>setvcpus KVM_name num</td>
<td>设置虚机的虚拟CPU个数。要求虚机不活动</td>
</tr>
<tr>
<td>autostart KVM_name</td>
<td>虚机将随宿主机一起启动  –disable  KVM_name：取消自动启动</td>
</tr>
</tbody></table>
<h3 id="9-4、参考文档"><a href="#9-4、参考文档" class="headerlink" title="9.4、参考文档"></a>9.4、参考文档</h3><p>virsh的详细命令解析 (<a target="_blank" rel="noopener" href="http://blog.chinaunix.net/uid-26284395-id-2888083.html">http://blog.chinaunix.net/uid-26284395-id-2888083.html</a>)</p>
<h1 id="ESXi简介"><a href="#ESXi简介" class="headerlink" title="ESXi简介"></a>ESXi简介</h1><p>在上面的【服务器虚拟化】的【全虚拟化】中，我们提到了ESXi。ESXi既是Hypervisor，也是一个服务器操作系统。ESXi是Vmware.lnc公司的vSphere软件集合的一个重要组成部分，下面详解介绍。</p>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32873934">vSphere, ESXi 和 vCenter 的区别</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Jmilk/article/details/52366733">vCenter 部件关系简介 &amp; 网络原理</a></p>
<h2 id="vSphere-ESXi-和-vCenter"><a href="#vSphere-ESXi-和-vCenter" class="headerlink" title="vSphere, ESXi 和 vCenter"></a>vSphere, ESXi 和 vCenter</h2><h3 id="Vmware-lnc"><a href="#Vmware-lnc" class="headerlink" title="Vmware.lnc"></a>Vmware.lnc</h3><p>VMware Inc. 是一家软件公司。它开发了许多产品，尤其是各种云解决方案 。他的云解决方案包括云产品，数据中心产品和桌面产品等</p>
<h3 id="vSphere"><a href="#vSphere" class="headerlink" title="vSphere"></a>vSphere</h3><p>vSphere 是在数据中心产品下的一套软件。vSphere 类似微软的 Office 办公套件，Office 办公套件包含了许多软件如Word, Excel, Access 等。和 Office 一样，vSphere 也是一个软件的集合。他包括了 vCenter, ESXi 和 vSphere 等。所以，这些软件联合起来就是 vSphere。vSphere 不是一个你可以安装使用的软件。它只是一个包含其它组件的集合。ESXi, vSphere client 和 vCeneter 都是 vSphere 的组件。</p>
<h3 id="ESXi"><a href="#ESXi" class="headerlink" title="ESXi"></a>ESXi</h3><p>ESXi是 vSphere 中最重要的一个组件。ESXi 是虚拟化服务。所有的虚拟机都是运行在 ESXi 服务上面。为了安装，管理和访问这些虚拟机，你需要另外的 vSphere 套件，也就是 vSphere client 或 vCenter。vSphere client允许管理员访问 ESXi 服务并管理虚拟机。vSphere client 是安装在客户机(也就是管理员的笔记本)上面。vSphere client 被用来连接 ESXi 服务器和管理任务。</p>
<h3 id="vCenter"><a href="#vCenter" class="headerlink" title="vCenter"></a>vCenter</h3><p>vCenter server 和 vSphere client 很像，但是它和功能更加强大。vCenter server 是安装在 Window 服务器或 Linux 服务器里面。VMware vCenter server 是一个中心化的管理应用。你可以通过它管理所有的虚拟机和 ESXi 物理机。vSphere client 可以通过访问 vCenter Server 来管理 EXSi 服务器。vCenter server 是一个企业级的产品，有许多企业级的功能，像 vMotion, VMware High Availability, VMware Update Manager 和 VMware Distributed Resource Scheduler(DRS)。你可以方便的通过 vCenter server 克隆存在的虚拟机。所以，vCenter 也是 vSphere 套件的一个重要组成部分。你需要单独购买 vCenter 的 license。</p>
<h3 id="关系总结"><a href="#关系总结" class="headerlink" title="关系总结"></a>关系总结</h3><p><img src="file:////Users/shenshawn/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image006.png" alt="img"></p>
<p>这个图描述了 vSphere 套间之前的关系。vSphere 是一个产品套件，ESXi 是安装在物理机上面的服务。vSphere Client 安装在笔记本或 PC 机上面，用来访问 ESXi 服务并安装和管理上面的虚拟机。vCenter Server 安装在了 ESXi 服务器的虚拟机里面。vCenter 也可以安装在单独的物理服务器上面，但是虚拟化应该会更好。vCenter 服务通常用在有很多 EXSi 服务和许多虚拟机的大规模环境中。vCenter 也可以使用 vSphere client 来管理。所以 vSphere client 可以在小环境中直接管理 ESXi 服务。也可以在大规模的环境中，通过 vCenter 服务间接管理 ESXi 服务。</p>

    </div>
     
    <div class="post-footer__meta"><p>更新于 2020-09-14</p></div> 
    <div class="post-meta__cats"></div> 
</article>


    <div class="nav">
        <div class="nav__prev">
            
                <a href="/heavyfish.github.io/2020/09/14/python/Python%E5%A6%82%E4%BD%95%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F/" class="nav__link">
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M589.088 790.624L310.464 512l278.624-278.624 45.248 45.248L400.96 512l233.376 233.376z" fill="#808080"></path></svg>
                    </div>
                    <div>
                        <div class="nav__label">
                            Previous Post
                        </div>
                        <div class="nav__title">
                            no title
                        </div>
                    </div>
                </a>
            
        </div>
        <div class="nav__next">
            
                <a href="/heavyfish.github.io/2020/09/10/Linux/Intel%20VMX%20technology/" class="nav__link">
                    <div>
                        <div class="nav__label">
                            Next Post
                        </div>
                        <div class="nav__title">
                            no title
                        </div>
                    </div>
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M434.944 790.624l-45.248-45.248L623.04 512l-233.376-233.376 45.248-45.248L713.568 512z" fill="#808080"></path></svg>
                    </div>
                </a>
            
        </div>
    </div>





</main>

            <footer class="footer">
    


    
     
 

 
    
        
        <p class="footer-copyright">
            Copyright © 2021 <a href="/heavyfish.github.io/">速查笔记</a>
        </p>
    
    
    <p>Powered by <a href="https://hexo.io" target="_blank">Hexo</a> | Theme - <a href="https://github.com/ChrAlpha/hexo-theme-cards" target="_blank">Cards</a></p>
</footer>

        </div>
         

 

 

 

  



 


    
 

 

 

 

 




    </body>
</html>
