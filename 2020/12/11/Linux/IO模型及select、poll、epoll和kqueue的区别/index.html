<!DOCTYPE html>
<html lang="zh-Hans">
    <head>
    <meta charset="utf-8">

    

    <!-- 渲染优化 -->
    <meta name="renderer" content="webkit">
    <meta name="force-rendering" content="webkit">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="HandheldFriendly" content="True" >
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!--icon-->

    
    
    
    
    


    <!-- meta -->


<title>no title | 速查笔记</title>





    <!-- OpenGraph -->
 
    <meta name="description" content="1、快速入门1、常见I&#x2F;O模型网络IO的本质是socket的读取，socket在linux系统被抽象为流，IO可以理解为对流的操作。刚才说了，对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段：   第一阶段：等待数据准备 (Waiting for the data">
<meta property="og:type" content="article">
<meta property="og:title" content="速查笔记">
<meta property="og:url" content="https://heavyfish.github.io/2020/12/11/Linux/IO%E6%A8%A1%E5%9E%8B%E5%8F%8Aselect%E3%80%81poll%E3%80%81epoll%E5%92%8Ckqueue%E7%9A%84%E5%8C%BA%E5%88%AB/index.html">
<meta property="og:site_name" content="速查笔记">
<meta property="og:description" content="1、快速入门1、常见I&#x2F;O模型网络IO的本质是socket的读取，socket在linux系统被抽象为流，IO可以理解为对流的操作。刚才说了，对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段：   第一阶段：等待数据准备 (Waiting for the data">
<meta property="og:locale">
<meta property="og:image" content="https://img-blog.csdn.net/20130810171529406?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2VuYmluZ29vbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="https://img-blog.csdn.net/20130810171540843">
<meta property="og:image" content="https://img-blog.csdn.net/20130810171559265">
<meta property="og:image" content="https://img-blog.csdn.net/20130810171606062">
<meta property="og:image" content="https://img-blog.csdn.net/20130810171622718">
<meta property="og:image" content="https://static.oschina.net/uploads/img/201604/21095604_vhHX.png">
<meta property="article:published_time" content="2020-12-11T13:39:34.294Z">
<meta property="article:modified_time" content="2020-12-14T07:31:14.055Z">
<meta property="article:author" content="Shenxr">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://img-blog.csdn.net/20130810171529406?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2VuYmluZ29vbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">


    
<link rel="stylesheet" href="/heavyfish.github.io/css/style/main.css">
 



    
    
        <link rel="stylesheet" id="hl-default-theme" href="/heavyfish.github.io/css/highlight/default.css" media="none" onload="if(getComputedStyle(document.documentElement).getPropertyValue('--color-mode').indexOf('dark')===-1)this.media='all'">
        
    

    
    

     

    <!-- custom head -->

<meta name="generator" content="Hexo 5.3.0"></head>

    <body>
        <div id="app">
            <header class="header">
    <div class="header__left">
        <a href="/heavyfish.github.io/" class="button">
            <span class="logo__text">Demo</span>
        </a>
    </div>
    <div class="header__right">
        
            <div class="navbar__menus">
                
                    <a href="/heavyfish.github.io/" class="navbar-menu button">首页</a>
                
                    <a href="/heavyfish.github.io/tags/" class="navbar-menu button">标签</a>
                
                    <a href="/heavyfish.github.io/categories/" class="navbar-menu button">分类</a>
                
                    <a href="/heavyfish.github.io/archives/" class="navbar-menu button">归档</a>
                
                    <a href="/heavyfish.github.io/friends/" class="navbar-menu button">友链</a>
                
                    <a href="/heavyfish.github.io/page/" class="navbar-menu button">Page</a>
                
            </div>
        
        
        

        
        

        

        
            <a class="dropdown-icon button" id="btn-dropdown" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width='24' height='24' fill="none" stroke="currentColor" stroke-width="0.7" stroke-linecap="round" stroke-linejoin="round"><path fill="currentColor" d="M3.314,4.8h13.372c0.41,0,0.743-0.333,0.743-0.743c0-0.41-0.333-0.743-0.743-0.743H3.314c-0.41,0-0.743,0.333-0.743,0.743C2.571,4.467,2.904,4.8,3.314,4.8z M16.686,15.2H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,15.2,16.686,15.2z M16.686,9.257H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,9.257,16.686,9.257z"></path></svg></a>
            <div class="dropdown-menus" id="dropdown-menus">
                
                    <a href="/heavyfish.github.io/" class="dropdown-menu button">首页</a>
                
                    <a href="/heavyfish.github.io/tags/" class="dropdown-menu button">标签</a>
                
                    <a href="/heavyfish.github.io/categories/" class="dropdown-menu button">分类</a>
                
                    <a href="/heavyfish.github.io/archives/" class="dropdown-menu button">归档</a>
                
                    <a href="/heavyfish.github.io/friends/" class="dropdown-menu button">友链</a>
                
                    <a href="/heavyfish.github.io/page/" class="dropdown-menu button">Page</a>
                
            </div>
        
    </div>
</header>


            <main class="main">
    

<div class="post-title">
    <h1 class="post-title__text">
        no title
    </h1>
    <div class="post-title__meta">
        <a href="/heavyfish.github.io/archives/2020/12/" class="post-meta__date button">2020-12-11</a>
        
 
        
    
    


 

 
    </div>
</div>



<article class="post content-card">
    <div class="post__header"></div>
    <div class="post__content">
        <h1 id="1、快速入门"><a href="#1、快速入门" class="headerlink" title="1、快速入门"></a>1、快速入门</h1><h2 id="1、常见I-O模型"><a href="#1、常见I-O模型" class="headerlink" title="1、常见I/O模型"></a>1、常见I/O模型</h2><p><code>网络IO的本质是socket的读取，socket在linux系统被抽象为流，IO可以理解为对流的操作</code>。刚才说了，对于一次IO访问（以read举例），<code>数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间</code>。所以说，当一个read操作发生时，它会经历两个阶段：</p>
<blockquote>
<ol>
<li>第一阶段：等待数据准备 (Waiting for the data to be ready)。</li>
<li>第二阶段：将数据从内核拷贝到进程中 (Copying the data from the kernel to the process)。</li>
</ol>
</blockquote>
<p>对于socket流而言，</p>
<blockquote>
<ol>
<li>第一步：通常涉及等待网络上的数据分组到达，然后被复制到内核的某个缓冲区。</li>
<li>第二步：把数据从内核缓冲区复制到应用进程缓冲区。</li>
</ol>
</blockquote>
<p>网络应用需要处理的无非就是两大类问题，<code>网络IO，数据计算</code>。相对于后者，网络IO的延迟，给应用带来的性能瓶颈大于后者。网络IO的模型大致有如下几种：</p>
<ul>
<li><p>blocking I/O</p>
</li>
<li><p>nonblocking I/O</p>
</li>
<li><p>I/O multiplexing (<code>select</code> and <code>poll</code>)</p>
</li>
<li><p>signal driven I/O (<code>SIGIO</code>)</p>
</li>
<li><p>asynchronous I/O (the POSIX <code>aio_</code>functions)—————<strong>异步IO模型最大的特点是 完成后发回通知</strong>。</p>
<p><strong>阻塞与否，取决于实现IO交换的方式。</strong></p>
<p><strong>异步阻塞是基于select，select函数本身的实现方式是阻塞的，而采用select函数有个好处就是它可以同时监听多个文件句柄</strong></p>
<p><strong>异步非阻塞直接在完成后通知，用户进程只需要发起一个IO操作然后立即返回，等IO操作真正的完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的IO读写操作，因为真正的IO读取或者写入操作已经由内核完成了。</strong></p>
</li>
</ul>
<table>
<thead>
<tr>
<th align="center">Mode</th>
<th align="center">Blocking</th>
<th align="center">Non-blocking</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Synchronous</td>
<td align="center">read/write</td>
<td align="center">read/write (O_NONBLOCK)</td>
</tr>
<tr>
<td align="center">Asynchronous</td>
<td align="center">I/O multiplexing (select/poll/epoll)</td>
<td align="center">AIO</td>
</tr>
</tbody></table>
<blockquote>
<p>注意：对于 I/O multiplexing 的定义是 同步 还是 异步 发现了不同的描述。都是基于角度不同而已，不用深究，关键在于对  I/O multiplexing 本身的理解</p>
</blockquote>
<h3 id="1-blocking-I-O"><a href="#1-blocking-I-O" class="headerlink" title="1 blocking I/O"></a><strong>1 blocking I/O</strong></h3><p>阻塞套接字，<code>application处于一种不再消费 CPU 而只是简单等待响应的状态</code>。下图是它调用过程的图示：</p>
<p><img src="https://img-blog.csdn.net/20130810171529406?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2VuYmluZ29vbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>重点解释下上图，下面例子都会讲到。首先application调用 recvfrom()转入kernel，注意kernel有2个过程，wait for data和copy data from kernel to user。直到最后copy complete后，recvfrom()才返回。此过程一直是阻塞的。</p>
<h3 id="2-nonblocking-I-O："><a href="#2-nonblocking-I-O：" class="headerlink" title="2 nonblocking I/O："></a><strong>2 nonblocking I/O：</strong></h3><p>与blocking I/O对立的，非阻塞套接字，调用过程图如下：</p>
<p><img src="https://img-blog.csdn.net/20130810171540843" alt="img"></p>
<p><code>同步非阻塞就是 “每隔一会儿瞄一眼进度条” 的轮询（polling）方式</code>。在这种模型中，<code>设备是以非阻塞的形式打开的</code>。这意味着 IO 操作不会立即完成，read 操作可能会返回一个错误代码，说明这个命令不能立即满足（EAGAIN 或 EWOULDBLOCK）。</p>
<p><code>也就是说非阻塞的recvform系统调用调用之后，进程并没有被阻塞，内核马上返回给进程，如果数据还没准备好，此时会返回一个error</code>。进程在返回之后，可以干点别的事情，然后再发起recvform系统调用。重复上面的过程，循环往复的进行recvform系统调用。<code>这个过程通常被称之为轮询</code>。轮询检查内核数据，直到数据准备好，再拷贝数据到进程，进行数据处理。**<code>需要注意，拷贝数据整个过程，进程仍然是属于阻塞的状态</code>**。</p>
<h3 id="3-I-O-multiplexing-select-and-poll"><a href="#3-I-O-multiplexing-select-and-poll" class="headerlink" title="3 I/O multiplexing (select and poll)"></a><strong>3 I/O multiplexing (select and poll)</strong></h3><p><strong>最常见的I/O复用模型，select。</strong></p>
<p><img src="https://img-blog.csdn.net/20130810171559265" alt="img"></p>
<p>由于同步非阻塞方式需要不断主动轮询，轮询占据了很大一部分过程，轮询会消耗大量的CPU时间，而 “后台” 可能有多个任务在同时进行，人们就想到了循环查询多个任务的完成状态，只要有任何一个任务完成，就去处理它。如果轮询不是进程的用户态，而是有人帮忙就好了。<code>那么这就是所谓的 “IO 多路复用”</code>。UNIX/Linux 下的 select、poll、epoll 就是干这个的（epoll 比 poll、select 效率高，做的事情是一样的）。</p>
<p>select调用是内核级别的，select轮询相对非阻塞的轮询的区别在于—<code>前者可以等待多个socket，能实现同时对多个IO端口进行监听</code>，当其中任何一个socket的数据准好了，<code>就能返回进行可读</code>，<code>然后进程再进行recvform系统调用，将数据由内核拷贝到用户进程，当然这个过程是阻塞的</code>。select或poll调用之后，会阻塞进程，与blocking IO阻塞不同在于，<code>此时的select不是等到socket数据全部到达再处理, 而是有了一部分数据就会调用用户进程来处理</code>。如何知道有一部分数据到达了呢？<code>监视的事情交给了内核，内核负责数据到达的处理</code>。</p>
<p><code>I/O复用模型会用到select、poll、epoll函数，这几个函数也会使进程阻塞，但是和阻塞I/O所不同的的，这几个函数可以同时阻塞多个I/O操作</code>。而且可以同时对多个读操作，多个写操作的I/O函数进行检测，直到有数据可读或可写时（注意不是全部数据可读或可写），才真正调用I/O操作函数。</p>
<p>对于多路复用，也就是轮询多个socket。<code>多路复用既然可以处理多个IO，也就带来了新的问题，多个IO之间的顺序变得不确定了</code>。</p>
<h3 id="4-signal-driven-I-O-SIGIO"><a href="#4-signal-driven-I-O-SIGIO" class="headerlink" title="4 signal driven I/O (SIGIO)"></a><strong>4 signal driven I/O (SIGIO)</strong></h3><p>信号驱动式I/O：首先我们允许Socket进行信号驱动IO,并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I/O操作函数处理数据。</p>
<p><img src="https://img-blog.csdn.net/20130810171606062" alt="img"></p>
<p>与**I/O multiplexing (select and poll)**相比，它的优势是，免去了select的阻塞与轮询，当有活跃套接字时，由注册的handler处理。</p>
<h3 id="5-asynchronous-I-O-the-POSIX-aio-functions"><a href="#5-asynchronous-I-O-the-POSIX-aio-functions" class="headerlink" title="5 asynchronous I/O (the POSIX aio_functions)"></a><strong>5 asynchronous I/O (the POSIX aio_functions)</strong></h3><p>相对于同步IO，异步IO不是顺序执行。<code>用户进程进行aio_read系统调用之后，无论内核数据是否准备好，都会直接返回给用户进程，然后用户态进程可以去做别的事情</code>。等到socket数据准备好了，内核直接复制数据给进程，<code>然后从内核向进程发送通知</code>。<code>IO两个阶段，进程都是非阻塞的</code>。</p>
<p>Linux提供了AIO库函数实现异步，但是用的很少。目前有很多开源的异步IO库，例如libevent、libev、libuv。Windows的IOCP 也实现了异步I/O</p>
<p><img src="https://img-blog.csdn.net/20130810171622718" alt="img"></p>
<h3 id="6-下面是以上五种模型的比较"><a href="#6-下面是以上五种模型的比较" class="headerlink" title="6 下面是以上五种模型的比较"></a><strong>6 下面是以上五种模型的比较</strong></h3><p><img src="https://static.oschina.net/uploads/img/201604/21095604_vhHX.png" alt="输入图片说明"></p>
<p>=====================分割线==================================</p>
<p>5种模型的比较比较清晰了，剩下的就是把select,epoll,iocp,kqueue按号入座那就OK了。</p>
<p><strong><em>\</em>select和iocp分别对应第3种与第5种模型，那么epoll与kqueue呢？其实也于select属于同一种模型，只是更高级一些，可以看作有了第4种模型的某些特性，如callback机制。**</strong></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p><strong>为什么epoll,kqueue比select高级？</strong> </p>
<p>答案是，他们无<strong>轮询</strong>。因为他们用callback取代了。想想看，当套接字比较多的时候，每次select()都要通过遍历FD_SETSIZE个Socket来完成调度,不管哪个Socket是活跃的,都遍历一遍。这会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，那就避免了轮询，这正是epoll与kqueue做的。</p>
<p><strong>windows or *nix （IOCP or kqueue/epoll）？</strong></p>
<p>诚然，Windows的IOCP非常出色，目前很少有支持<strong>asynchronous I/O</strong>的系统，但是由于其系统本身的局限性，大型<a target="_blank" rel="noopener" href="http://detail.zol.com.cn/server_index/subcate31_list_1.html">服务器</a>还是在UNIX下。而且正如上面所述，kqueue/epoll 与 IOCP相比，就是多了一层从内核copy数据到应用层的阻塞，从而不能算作<strong>asynchronous I/O类。</strong>但是，这层小小的阻塞无足轻重，kqueue与epoll已经做得很优秀了。</p>
<p><strong>提供一致的接口，IO Design Patterns</strong></p>
<p>实际上，不管是哪种模型，都可以抽象一层出来，提供一致的接口，广为人知的有ACE,<strong>Libevent（基于reactor模式）</strong>这些，他们都是跨平台的，而且他们自动选择最优的I/O复用机制，用户只需调用接口即可。说到这里又得说说2个设计模式，Reactor and <em>Proactor。见：</em><a target="_blank" rel="noopener" href="http://blog.csdn.net/wenbingoon/article/details/9880365">Reactor模式–VS–Proactor模式</a>。Libevent是Reactor模型，ACE提供Proactor模型。实际都是对各种I/O复用机制的封装。</p>
<p><strong><em>\</em>总结一些重点：**</strong></p>
<ol>
<li><strong><em>\</em>只有IOCP是asynchronous I/O，其他机制或多或少都会有一点阻塞。**</strong></li>
<li><strong><em>\</em>select低效是因为每次它都需要轮询。但低效也是相对的，视情况而定，也可通过良好的设计改善**</strong></li>
<li><strong><em>\</em>epoll, kqueue、select是Reacor模式，IOCP是Proactor模式。**</strong></li>
<li><strong><em>\</em>java nio包是select模型。。**</strong></li>
</ol>
<h2 id="2、epoll-与select的区别"><a href="#2、epoll-与select的区别" class="headerlink" title="2、epoll 与select的区别"></a>2、epoll 与select的区别</h2><p><strong><em>\</em>1. 使用多进程或者多线程，但是这种方法会造成程序的复杂，而且对与进程与线程的创建维护也需要很多的开销。（Apache服务器是用的子进程的方式，优点可以隔离用户）  **（同步阻塞IO）*\</strong>***</p>
<p><strong><em>\</em>2.一种较好的方式为\</strong>*<em>I/O多路复用*</em>*<em>（I/O multiplexing）（先构造一张有关描述符的列表（epoll中为队列），然后调用一个函数，直到这些描述符中的一个准备好时才返回，返回时告诉进程哪些I/O就绪。select和epoll这两个机制都是多路I/O机制的解决方案，select为POSIX标准中的，而epoll为Linux所特有的。***</em></p>
<h3 id="区别（epoll相对select优点）主要有三："><a href="#区别（epoll相对select优点）主要有三：" class="headerlink" title="区别（epoll相对select优点）主要有三："></a><strong>区别（epoll相对select优点）主要有三：</strong></h3><p><strong>1</strong>.select的句柄数目受限，在linux/posix_types.h头文件有这样的声明：#define __FD_SETSIZE  1024 表示select最多同时监听1024个fd。而epoll没有，它的限制是最大的打开文件句柄数目。</p>
<p>**2.**epoll的最大好处是不会随着FD的数目增长而降低效率，在selec中采用轮询处理，其中的数据结构类似一个数组的数据结构，而epoll是维护一个队列，直接看队列是不是空就可以了。epoll只会对”活跃”的socket进行操作—这是因为在内核实现中epoll是根据每个fd上面的callback函数实现的。那么，只有”活跃”的socket才会主动的去调用 callback函数（把这个句柄加入队列），其他idle状态句柄则不会，在这点上，epoll实现了一个”伪”AIO。但是如果绝大部分的I/O都是“活跃的”，每个I/O端口使用率很高的话，epoll效率不一定比select高（可能是要维护队列复杂）。</p>
<p><strong>3</strong>.使用mmap加速内核与用户空间的消息传递。无论是select,poll还是epoll都需要内核把FD消息通知给用户空间，如何避免不必要的内存拷贝就很重要，在这点上，epoll是通过内核与用户空间mmap同一块内存实现的。</p>
<h3 id="关于epoll工作模式ET，LT"><a href="#关于epoll工作模式ET，LT" class="headerlink" title="关于epoll工作模式ET，LT"></a><strong>关于epoll工作模式ET，LT</strong></h3><p>epoll有两种工作方式</p>
<p>ET：Edge Triggered，边缘触发。仅当状态发生变化时才会通知，epoll_wait返回。换句话，就是对于一个事件，只通知一次。且只支持非阻塞的socket。</p>
<p>LT：Level Triggered，电平触发（默认工作方式）。类似select/poll,只要还有没有处理的事件就会一直通知，以LT方式调用epoll接口的时候，它就相当于一个速度比较快的poll.支持阻塞和不阻塞的socket。</p>
<h2 id="3、Linux并发网络编程模型"><a href="#3、Linux并发网络编程模型" class="headerlink" title="3、Linux并发网络编程模型"></a>3、Linux并发网络编程模型</h2><p>  1  Apache 模型，简称 PPC （ Process Per Connection ，）:为每个连接分配一个进程。主机分配给每个连接的时间和空间上代价较大，并且随着连接的增多，大量进程间切换开销也增长了。很难应对大量的客户并发连接。</p>
<p>  2  TPC 模型（ Thread Per Connection ）：每个连接一个线程。和PPC类似。</p>
<p>  3  select 模型：I/O多路复用技术。</p>
<p>​    .1 每个连接对应一个描述。select模型受限于 FD_SETSIZE即进程最大打开的描述符数linux2.6.35为1024,实际上linux每个进程所能打开描数字的个数仅受限于内存大小，然而在设计select的系统调用时，却是参考FD_SETSIZE的值。可通过重新编译内核更改此值，但不能根治此问题，对于百万级的用户连接请求  即便增加相应 进程数， 仍显得杯水车薪呀。</p>
<p>   .2select每次都会扫描一个文件描述符的集合，这个集合的大小是作为select第一个参数传入的值。但是每个进程所能打开文件描述符若是增加了 ，扫描的效率也将减小。</p>
<p>   .3内核到用户空间，采用内存复制传递文件描述上发生的信息。 </p>
<p> 4 poll 模型：I/O多路复用技术。poll模型将不会受限于FD_SETSIZE，因为内核所扫描的文件 描述符集合的大小是由用户指定的，即poll的第二个参数。但仍有扫描效率和内存拷贝问题。</p>
<p> 5 pselect模型：I/O多路复用技术。同select。</p>
<p> 6 epoll模型：</p>
<p>  .1)无文件描述字大小限制仅与内存大小相关</p>
<p>  .2)epoll返回时已经明确的知道哪个socket fd发生了什么事件，不用像select那样再一个个比对。</p>
<p> .3)内核到用户空间采用共享内存方式，传递消息。</p>
<h2 id="4、FAQ"><a href="#4、FAQ" class="headerlink" title="4、FAQ"></a>4、FAQ</h2><p>1、单个epoll并不能解决所有问题，特别是你的每个操作都比较费时的时候，因为epoll是串行处理的。 所以你有还是必要建立线程池来发挥更大的效能。 </p>
<p>2、如果fd被注册到两个epoll中时，如果有事件发生则两个epoll都会触发事件。</p>
<p>3、如果注册到epoll中的fd被关闭，则其会自动被清除出epoll监听列表。<br>4、如果多个事件同时触发epoll，则多个事件会被联合在一起返回。<br>5、epoll_wait会一直监听epollhup事件发生，所以其不需要添加到events中。<br>6、为了避免大数据量io时，et模式下只处理一个fd,其他fd被饿死的情况发生。linux建议可以在fd联系到的结构中增加ready位，然后epoll_wait触发事件之后仅将其置位为ready模式，然后在下边轮询ready fd列表。</p>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wenbingoon/article/details/9004512">IO模型及select、poll、epoll和kqueue的区别</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/486b0965c296">聊聊Linux 五种IO模型</a></p>
<h1 id="2、select-与-epoll"><a href="#2、select-与-epoll" class="headerlink" title="2、select 与 epoll"></a>2、select 与 epoll</h1><p>我们常学习的实现 I/O 多路复用的系统调用有四种：</p>
<ul>
<li>select （select 和 pool 提供基本相同的功能。它们只是在细节上有所不同）</li>
<li>pool</li>
<li>epoll    （Linux 专属的系统调用）</li>
<li>kqueue  （被 FreeBSD、NetBSD、OpenBSD、macOS等系统支持）</li>
</ul>
<p>我们会细致比较的是 select 与 epoll。</p>
<h2 id="2-1、复用技术和I-O复用"><a href="#2-1、复用技术和I-O复用" class="headerlink" title="2.1、复用技术和I/O复用"></a>2.1、<strong>复用技术和I/O复用</strong></h2><ul>
<li><strong><em>复用的概念</em></strong></li>
</ul>
<p>复用技术(multiplexing)并不是新技术而是一种设计思想，在通信和硬件设计中存在频分复用、时分复用、波分复用、码分复用等，在日常生活中复用的场景也非常多，因此不要被专业术语所迷惑。从本质上来说，复用就是为了解决<em>有限资源和过多使用者的不平衡问题</em>，<strong>复用技术的理论基础是资源的可释放性</strong>。</p>
<ul>
<li><strong><em>资源的可释放性</em></strong></li>
</ul>
<p>举个实际生活的例子：*<strong>不可释放场景</strong>：<em>ICU病房的呼吸机作为有限资源，病人一旦占用且在未脱离危险之前是无法放弃占用的，因此不可能几个情况一样的病人轮流使用。**</em>可释放场景*<em>：</em>对于一些其他资源比如医护人员就可以实现对多个病人的同时监护，理论上不存在一个病人占用医护人员资源不释放的场景。</p>
<ul>
<li><strong><em>理解IO复用</em></strong></li>
</ul>
<p>*<strong>I/O的含义</strong>：*在计算机领域常说的IO包括磁盘IO和网络IO，我们所说的IO复用主要是指网络IO，在Linux中一切皆文件，因此网络IO也经常用文件描述符FD来表示。</p>
<p>*<strong>复用的含义</strong>：*那么这些文件描述符FD要复用什么呢？在网络场景中复用的就是任务处理线程，所以简单理解就是多个IO共用1个线程。</p>
<p><em>I<strong>O复用的可行性</strong>：</em>IO请求的基本操作包括read和write，由于网络交互的本质性，必然存在等待，换言之就是整个网络连接中FD的读写是交替出现的，时而可读可写，时而空闲，所以IO复用是可用实现的。</p>
<p>综上认为，IO复用技术就是协调多个可释放资源的FD交替共享任务处理线程完成通信任务，实现多个fd对应1个任务处理线程。</p>
<p>现实生活中IO复用就像一只边牧管理几百只绵羊一样。</p>
<ul>
<li><strong><em>IO复用的设计原则和产生背景</em></strong></li>
</ul>
<p><strong><em>高效IO复用机制要满足</em>：协调者消耗最少的系统资源、最小化FD的等待时间、最大化FD的数量、任务处理线程最少的空闲、多快好省完成任务等</strong>。</p>
<p>在网络并发量非常小的原始时期，即使per req per process地处理网络请求也可以满足要求，但是随着网络并发量的提高，原始方式必将阻碍进步，所以就刺激了IO复用机制的实现和推广。</p>
<h2 id="2-2、select-和-epoll-的任务"><a href="#2-2、select-和-epoll-的任务" class="headerlink" title="2.2、select 和 epoll 的任务"></a>2.2、select 和 epoll 的任务</h2><p>要比较epoll相比较select高效在什么地方，就需要比较二者做相同事情的方法。</p>
<p>要完成对I/O流的复用需要完成如下几个事情：</p>
<p><strong>1.用户态怎么将文件句柄传递到内核态？</strong></p>
<p><strong>2.内核态怎么判断I/O流可读可写？</strong></p>
<p><strong>3.内核怎么通知监控者有I/O流可读可写？</strong></p>
<p><strong>4.监控者如何找到可读可写的I/O流并传递给用户态应用程序？</strong></p>
<p><strong>5.继续循环时监控者怎样重复上述步骤？</strong></p>
<p>搞清楚上述的步骤也就能解开epoll高效的原因了。</p>
<h3 id="select的做法："><a href="#select的做法：" class="headerlink" title="select的做法："></a><strong>select的做法：</strong></h3><p>步骤1的解法：select创建3个文件描述符集，并将这些文件描述符拷贝到内核中，这里限制了文件句柄的最大的数量为1024（注意是全部传入—第一次拷贝）；</p>
<p>步骤2的解法：内核针对读缓冲区和写缓冲区来判断是否可读可写,这个动作和select无关；</p>
<p>步骤3的解法：内核在检测到文件句柄可读/可写时就产生中断通知监控者select，select被内核触发之后，就返回可读可写的文件句柄的总数；</p>
<p>步骤4的解法：select会将之前传递给内核的文件句柄（即fd_set）再次从内核传到用户态（第2次拷贝），select返回给用户态的只是可读可写的文件句柄总数，再使用FD_ISSET宏函数来检测哪些文件I/O可读可写（遍历）；</p>
<p>步骤5的解法：select对于事件的监控是建立在内核的修改之上的，也就是说经过一次监控之后，内核会修改位，因此再次监控时需要再次从用户态向内核态进行拷贝（第N次拷贝）</p>
<h3 id="epoll的做法："><a href="#epoll的做法：" class="headerlink" title="epoll的做法："></a><strong>epoll的做法：</strong></h3><p>步骤1的解法：首先执行epoll_create在内核专属于epoll的高速cache区，并在该缓冲区建立红黑树和就绪链表，用户态传入的文件句柄将被放到红黑树中（第一次拷贝）。</p>
<p>步骤2的解法：内核针对读缓冲区和写缓冲区来判断是否可读可写，这个动作与epoll无关；</p>
<p>步骤3的解法：epoll_ctl执行add动作时除了将文件句柄放到红黑树上之外，还向内核注册了该文件句柄的回调函数，内核在检测到某句柄可读可写时则调用该回调函数，回调函数将文件句柄放到就绪链表。</p>
<p>步骤4的解法：epoll_wait只监控就绪链表就可以，如果就绪链表有文件句柄，则表示该文件句柄可读可写，并返回到用户态（少量的拷贝）；</p>
<p>步骤5的解法：由于内核不修改文件句柄的位，因此只需要在第一次传入就可以重复监控，直到使用epoll_ctl删除，否则不需要重新传入，因此无多次拷贝。</p>
<p>简单说：epoll是继承了select/poll的I/O复用的思想，并在二者的基础上从监控IO流、查找I/O事件等角度来提高效率，具体地说就是内核句柄列表、红黑树、就绪list链表来实现的。</p>
<h2 id="2-3、epoll-详解"><a href="#2-3、epoll-详解" class="headerlink" title="2.3、epoll 详解"></a>2.3、epoll 详解</h2><p>先简单回顾下如何使用C库封装的3个epoll系统调用吧。</p>
<ol>
<li><strong>int</strong> epoll_create(<strong>int</strong> size);  </li>
<li><strong>int</strong> epoll_ctl(<strong>int</strong> epfd, <strong>int</strong> op, <strong>int</strong> fd, <strong>struct</strong> epoll_event *event);  </li>
<li><strong>int</strong> epoll_wait(<strong>int</strong> epfd, <strong>struct</strong> epoll_event *events,<strong>int</strong> maxevents, <strong>int</strong> timeout);  </li>
</ol>
<p>使用起来很清晰：</p>
<p>A.epoll_create建立一个epoll对象。参数size是内核保证能够正确处理的最大句柄数，多于这个最大数时内核可不保证效果。</p>
<p>B.epoll_ctl可以操作上面建立的epoll，例如，将刚建立的socket加入到epoll中让其监控，或者把 epoll正在监控的某个socket句柄移出epoll，不再监控它等等(<strong>也就是将I/O流放到内核</strong>)。</p>
<p>C.epoll_wait在调用时，在给定的timeout时间内，当在监控的所有句柄中有事件发生时，就返回用户态的进程（<strong>也就是在内核层面捕获可读写的I/O事件</strong>）。</p>
<p>从上面的调用方式就可以看到epoll比select/poll的优越之处：</p>
<p>因为后者每次调用时都要传递你所要监控的所有socket给select/poll系统调用，这意味着需要将用户态的socket列表copy到内核态，如果以万计的句柄会导致每次都要copy几十几百KB的内存到内核态，非常低效。而我们调用epoll_wait时就相当于以往调用select/poll，但是这时却不用传递socket句柄给内核，因为内核已经在epoll_ctl中拿到了要监控的句柄列表。</p>
<p>====&gt;<em>select监控的句柄列表在用户态，每次调用都需要从用户态将句柄列表拷贝到内核态，但是epoll中句柄就是建立在内核中的，这样就减少了内核和用户态的拷贝，高效的原因之一。</em></p>
<p><strong>epoll综合的执行过程：</strong> </p>
<p>​      如此，一棵红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。执行epoll_create时，创建了红黑树和就绪链表，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据。执行epoll_wait时立刻返回准备就绪链表里的数据即可。</p>
<p><strong>epoll水平触发和边缘触发的实现：</strong></p>
<p>​     当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_wait，会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表， 最后，epoll_wait干了件事，就是检查这些socket，如果不是ET模式（就是LT模式的句柄了），并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了，所以，非ET的句柄，只要它上面还有事件，epoll_wait每次都会返回。而ET模式的句柄，除非有新中断到，即使socket上的事件没有处理完，也是不会次次从epoll_wait返回的。</p>
<p><em>====&gt;区别就在于epoll_wait将socket返回到用户态时是否清空就绪链表。</em></p>
<h2 id="参考文档："><a href="#参考文档：" class="headerlink" title="参考文档："></a>参考文档：</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/87843750">深入理解IO复用之epoll-知乎-静海听风</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/20122137">epoll 或者 kqueue 的原理是什么？</a>-知乎</p>
<p><a target="_blank" rel="noopener" href="https://daniel.haxx.se/docs/poll-vs-select.html">poll vs select vs event-based</a></p>

    </div>
     
    <div class="post-footer__meta"><p>updated at 2020-12-14</p></div> 
    <div class="post-meta__cats"></div> 
</article>


    <div class="nav">
        <div class="nav__prev">
            
                <a href="/heavyfish.github.io/2020/12/14/Linux/Nginx/" class="nav__link">
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M589.088 790.624L310.464 512l278.624-278.624 45.248 45.248L400.96 512l233.376 233.376z" fill="#808080"></path></svg>
                    </div>
                    <div>
                        <div class="nav__label">
                            Previous Post
                        </div>
                        <div class="nav__title">
                            no title
                        </div>
                    </div>
                </a>
            
        </div>
        <div class="nav__next">
            
                <a href="/heavyfish.github.io/2020/11/24/k8s/%E4%BB%8B%E7%BB%8DKubernetes%E5%AE%B9%E5%99%A8%E5%AD%98%E5%82%A8%E6%8E%A5%E5%8F%A3%EF%BC%88CSI%EF%BC%89/" class="nav__link">
                    <div>
                        <div class="nav__label">
                            Next Post
                        </div>
                        <div class="nav__title">
                            no title
                        </div>
                    </div>
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M434.944 790.624l-45.248-45.248L623.04 512l-233.376-233.376 45.248-45.248L713.568 512z" fill="#808080"></path></svg>
                    </div>
                </a>
            
        </div>
    </div>





</main>

            <footer class="footer">
    


    
     
 

 
    
        
        <p class="footer-copyright">
            Copyright © 2021 <a href="/heavyfish.github.io/">速查笔记</a>
        </p>
    
    
    <p>Powered by <a href="https://hexo.io" target="_blank">Hexo</a> | Theme - <a href="https://github.com/ChrAlpha/hexo-theme-cards" target="_blank">Cards</a></p>
</footer>

        </div>
         

 

 

 

  



 


    
 

 

 

 

 




    </body>
</html>
